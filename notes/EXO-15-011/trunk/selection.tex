\section{Trigger \& Selection}
\label{selection}

\subsection{Trigger}


In Run~2 the RA1 analysis retains the low-thresholds of Run~1 with developments 
to the trigger selection, maintaining sensitivity to signatures of new physics with hadronic 
energies as low as $\scalht = 200$ GeV. This in part is achieved by a migration to PF-based 
jet reconstruction within the HLT which, in conjunction with a reduction of clustering radius 
parameter $\Delta R = 0.4$, provides improvements in jet energy resolution in high-pileup 
conditions and mitigates the effects of pileup contamination within the jet cone.

This analysis utilises a range of triggers for the selection of events in the hadronic signal region
to provide coverage over a wide range of event topologies. 
Events in the hadronic monojet signal region are selected with the use of a single \mht-\met cross-trigger.

The hadronic multijet signal region is selected by a suite of $\scalht$-$\alphat$ cross-triggers 
with a requirement on the average \pt of the leading two jets, $\pt^{\rm \left<j1,j2\right>}$
The use of a dijet average threshold provides an improved  suppression of QCD multijet events within the trigger,
enabling looser \alphat thresholds to be utilised whilst maintaining acceptance to events exhibiting asymmetric jet 
topologies such as monojet-like signatures of compressed spectrum and DM models. It was found that a dijet average
threshold of 90 \GeV ensured the optimum performance when balancing efficiency and rate, with both the $\scalht$ and $\alphat$
thresholds across all jet toplogies. 

The $\scalht$-$\alphat$ triggers seed all signal multijet offline bins in the range for $\scalht > 200$ \GeV. 
Above $\scalht > 800$ an additional pure \scalht trigger, \verb!HLT_PFHT800!, is also utilised which has
 no explicit dependence on $\alphat$ or dijet average threshold. 

These triggers form the primary signal selection menu for the analysis, in addition a set of secondary triggers with 
higher thresholds were in operation to provide redundancy for higher luminosity scenarios. In Run 2 however the primary 
triggers sufficiently suppressed trigger rates such that the use of secondary triggers in the analysis are not required.


Table~\ref{tab:2015_Hadronic_Signal_Triggers} gives the triggers used in this analysis.



% TABLE : 2015 triggers
%----------------------------------------------------------------------
\begin{table}[h!]
\topcaption{Trigger thresholds of the Level-1, calorimeter prefilter and final PF-trigger decision for
 the primary HLT paths for the hadronic signal region in the $\lumi=7\times10^{33}\cm^{-2}{\rm s}^{-1}$ scenario.
 Higher threshold Level-1 \scalht and \met seeds are utilised for the higher luminosity scenario. }
\footnotesize
\centering
\begin{tabular}{c|cccc} 
\hline
\hline
HLT path     & L1 seed & HLT calo-prefilter & HLT PF-filter                                                \\
    &        & ($\scalht$, $\alphat$', $\pt^{\rm \left<j1,j2\right>}$, \met) & ($\scalht$, $\alphat$, $\pt^{\rm \left<j1,j2\right>}$, \met) \\ %& (Hz) \\[0.7 ex] 
\hline
{\scriptsize \verb!HLT_PFHT200_PFDijetAve90_AlphaT0p57!} & {\scriptsize \verb!HTT125 OR ETM50!} & 150, 0.540, 70, - & 200, 0.570, 90, - \\ %& \\ % 11.0 $\pm$ 3.0 \\
{\scriptsize \verb!HLT_PFHT250_PFDijetAve90_AlphaT0p55!} & {\scriptsize \verb!HTT125 OR ETM50!} & 200, 0.535, 70, - & 250, 0.550, 90, - \\ %& \\ % 8.5  $\pm$ 3.0 \\
{\scriptsize \verb!HLT_PFHT300_PFDijetAve90_AlphaT0p53!} & {\scriptsize \verb!HTT125 OR ETM50!} & 250, 0.525, 70, - & 300, 0.530, 90, - \\ %& \\ % 9.5  $\pm$ 3.0 \\
{\scriptsize \verb!HLT_PFHT350_PFDijetAve90_AlphaT0p52!} & {\scriptsize \verb!HTT125 OR ETM50!} & 300, 0.520, 70, - & 350, 0.520, 90, - \\ %& \\ % 10.0 $\pm$ 3.0 \\
{\scriptsize \verb!HLT_PFHT400_PFDijetAve90_AlphaT0p51!} & {\scriptsize \verb!HTT125 OR ETM50!} & 370, 0.510, 70, - & 400, 0.510, 90, - \\ %& \\ % 13.5 $\pm$ 3.5 \\ \\ %\hline \\ %
{\scriptsize \verb!HLT_PFHT800!}                         & {\scriptsize \verb!HTT175!}          & 650, -, -, -      & 800, -, -, -, -   \\ %& \\ % 13.5 $\pm$ 3.5 \\ \\ %\hline \\ %
{\scriptsize \verb!HLT_PFMETNoMu90_PFMHTNoMu90_IDTight!} & {\scriptsize \verb!ETM50!}           &  -, -, -, -, 65   & -, -, -, -, 90    \\
%% L1sL1ETM70ORETM60ORETM50
%% hltMET, MHT 65
\hline
\hline
\end{tabular}
\label{tab:2015_Hadronic_Signal_Triggers}
\end{table}


The analysis selection are defined such that we are oprating on or close to the trigger plateaus. Trigger weights are measured and applied
as appropriate. Details regarding trigger efficiencies and turn-on curves can be found in Ref.~\cite{alpaTnote}


Control regions are selected in hadronic, electron- and muon final states. For the hadronic region 
prescaled $\scalht$ triggers, and a prescaled  $\scalht$-$\alphat$ cross-trigger, are utilised.
These triggers share the same Level-1 seeds and $\scalht$ threshold of the signal cross-triggers and are similarly each mapped 
to a unique offline bin. The efficiency of these triggers are similarly measured from an electron 
reference trigger in addition to an independent measurement with the \verb!HLT_Physics! 
minimum bias trigger.


The non-hadronic control regions are seeded by the lowest-threshold unprescaled triggers available in the given run scenario. In the high-luminosity scenario The 
\mj and \mmj control samples are selected with the \verb!HLT_IsoMu20! trigger, and the \gj control sample by the \verb!HLT_Photon175! trigger. 
%The \ej and \eej control samples may be seeded by the \verb!HLT_Ele23_eta2p1_WPLoose_Gsf! trigger.

The efficiency of the control triggers is measured with data-driven methods (provided by the relevant POG). The tag and probe method is used in the measurement of
efficiencies of the muon and electron triggers and a loose photon reference trigger  is utilised in the measurement of the photon trigger efficiency. For the results in 
this note for the muon control regions the emulated trigger bit in MC is used to simulate  the trigger. An offline \Pt requirement of 200\GeV is made on the photon
to ensure it is in the efficiency plateau of the trigger.



%%____________________________________________________________________________||



\subsection{Object Defintions}

All selection are based on POG recommended object definitions. Specificially:

\begin{itemize}
  \item{\bf Jets}: This analysis uses particle-flow (PF) candidates clustered by the anti-$k_{T}$ jet clustering algorithm \cite{Cacciari:2008gp} using a cone radius of 0.4. All corrections
    like charged hadron subtraction and jet energy are applied. The ``loose'' working point jet-ID selection is used.
    
  \item{\bf $b$-quarks:} The Combined Secondary Vertex tagger V2 $b$-jet identification algorithm is used ith its ``medium'' working point. This results in a light-quark mis-tag rate of $\sim$1 \%  and a $b$-tag signal efficiency of about 80 \%.   

  \item{\bf Muons} Muons are identified according to the ``medium'' working point definition of the recommended identification algorithm, which provides $\sim$ 98 $\%$ efficiency. 
Muons are also required to be isolated, i.e. with a low activity in the vicinity of their track. In the hadronic signal region, a PF-based relative isolation is used with a variable cone size, which is referred to as ``mini-isolation''. 

  \item{\bf Photons} Photons that are identified by the cut-based photon identification algorithm \cite{photon-id} and that fullfill the ``tight'' working point ($\sim$ 71 $\%$ efficiency) are used. The and required to be well isolated.  A PF-based isolation is used with a cone size $\Delta R$ $<$ 0.3  and correction to remove pile up effects are applied.


  \item{\bf Electrons} Isolated electrons are identified according to the ``loose'' working point definition ($\sim$ 90 $\%$ efficiency)  of the cut-based identification \cite{electron-id} for the purpose of the electron veto in the signal region, while the ``tight'' working point ($\sim$ 70 $\%$ efficiency) is used for the selection of electron in the corresponding control regions. Similarly to muons a PF-based isolation \cite{pf-photon} is used in the hadronic signal regions with a cone size determined by the mini isolation algorithm and corrections are applied to remove the effects of pileup.

  \item{\bf Isolated Tracks}  A single isolated track (SIT) can be used to identify leptonically decaysing $W$ bosons through their leptonic decays and single prong tau decays.
    A SIT comprises a charged PF candidate with $\Pt > 10 \gev$, $\Delta z(\mathrm{track}, \mathrm{PV}) < 0.05 \, \mathrm{cm}$  and with a relative isolation smaller than 0.1, where the isolation is determined from the sum of the \Pt of the charged PF candidates within $\Delta R < 0.3$.

 \item{\bf Missing transverse momentum} Missing transverse momentum (\met) is defined as the opposite of the vector sum of the transverse momentum of all particle-flow candidates in the event. Type-I \met correction \cite{Khachatryan:2014gga} are applied.


\end{itemize}


Muon, Electrons, Photons aren taus are vetoed in the definition of the hadronic signal region. Additional details of object selections and correction are described in Ref.~\cite{alphaTnote}

\subsection{Selections}


This section first outlines the set of ``pre-selection'' requirements that are common to all signal and control regions, before defining the
selection criteria that are specific to each region. 
All ``MET filters'' recommended by the JetMET POG and SUSY PAG are applied to remove beam- and detector-related effects. 
Jets are required to satisfy $\PT>40\gev$ and $|\eta|<3.0$. Events containing jets in the forward region that
satisfy the requirements $\PT>40\gev$ and $|\eta|>3.0$ are rejected in order to control background contributions from SM processes, without
introducing a significant reduction in signal acceptance. 

The jets that are selected are used in the calculation of all jet-based event-level variables, such as \HT, \mht, and \alphat.

We require the leading jet to fullfill $\PT > 100\gev$ and $|\eta|<2.5$.  Events are then classified based on the second leading (trailing) jet. 
If the trailing jets satisfies $\PT > 100\gev$  events are assigned to a ``symmetric'' \njet category. If the second
jet satisfies $40 < \PT < 100\gev$ events are assigned to an ``asymmetric'' \njet category. Finally, if there is no second leading
jet with $\PT>40\gev$, events are assigned to the ``mono-jet'' category. 

These asymmetric and mono-jet categories - more than doubling the analysis signal regions and covering challenging kinematic regions - have been added to
the analysis to help improve acceptance to a range of DM models.



Events in the hadronic signal and all control regions (described
below) are categorised identically and according to the number of jets
(\njet) reconstructed in each event and the number of jets identified
as originating from bottom quarks (\nb) in each event. 

As a baseline, the resulting sub-samples comprise events containing exactly one, two,
three, four, or at least five jets. These are further split into the aforementioned  ``monojet'', 
``symmetric'' or ``asymmetric'' \njet categories.

Events are also categorised according to the the number of b-tagged jets (``b-jets''), by
requiring exactly zero, one, two, or at least three b-tagged jets. 

Events are required to have significant hadronic activity by requiring
$\scalht > 200\GeV$.  Events in all samples are binned identically, according to the
\HT variable. The choice of binning in \HT is driven primarily by the trigger strategy: 50\gev
bins in the range $200 < \HT < 400\gev$, 100\gev bins in the range $400 < \HT < 600\gev$, a 200\gev bin $600<\HT<800\gev$ and a final 
inclusive bin $\HT > 800\gev$.

Events are also required to have appreciable missing hadronic energy by requiring $\mht>130\gev$. This ensures that all events used within
the analysis have a degree of missing energy similar to that required in the signal region via an \alphat cut. 

An overview of all selection criteria is given in Table~\ref{tab:pre-selections}.

\begin{table}[h!]
  \topcaption{Summary of the pre-selection criteria.}
  \label{tab:pre-selections}
  \centering
  \footnotesize
  \begin{tabular}{ ll }
    \hline
    \hline
    Selection                     & Requirement                                                                          \\
    \hline
    ``MET filters''               & Primary Vertex, CSC Beam Halo, HBHE Noise and Isolation, ECAL Endcap SC Noise        \\
    Jet acceptance                & $\PT > 40\gev$, $|\eta| < 3$                                                         \\
%    \njet                         & $\geq2$                                                                \\
    Lead jet acceptance           & $\PT > 100\gev$, $|\eta| <    2.5$                                     \\
    Second jet acceptance         & $\PT > 100\gev$ \texttt{OR} $40 < \PT < 100\gev$                       \\
    Loosest \HT requirement       & $\HT > 200\gev$                                                        \\
    Loosest \mht requirement      & $>130\gev$                                                     \\  
    Baseline \HT binning          & 200--250, 250--300, 300--350, 350--400, 400--500, 500--600, 600--800, $>$800\gev \\
    Baseline \njet multiplicities & 1 (mono-jet), 2, 3, 4, $\geq$5 (both symmetric and asymmetric)                       \\
    Baseline \nb multiplicities   & 0, 1, 2, $\geq3$ ($\nb \leq \njet$)                                    \\
    \hline
    \hline
  \end{tabular}
\end{table}








%marcelle
\subsection{The hadronic signal region}

To select a sample of events in the hadronic final state and to suppress SM processes with genuine \met from neutrinos, events
containing an isolated electron with $\pt > 10\GeV$ and $|\eta| < 2.5$ or an isolated muon with $\pt > 10\GeV$ and $|\eta| < 2.5$ are
vetoed. Further, to reduce the ``lost leptons'' backgrounds from \wj and \ttbar, events containing single isolated tracks with $\pt >
10\GeV$ and $|\eta| < 2.5$ are rejected. In the case of the single and di-lepton control samples, a further
requirement is made such that events are not vetoed due to the presence of a track from the well identified leptons, by requiring
$\Delta R(\textrm{track},\textrm{lepton}) > 0.02$. Finally, to select a purely hadronic topology and to allow for a 
orthogonal control region, events are vetoed in which an isolated photon with $\pt > 25\GeV$ and $|\eta| < 2.5$ is identified.


%Background events from multijet production populate the region $\alphat \lesssim 0.5$ and therefore can be rejected with very high
%efficiency by requiring an appropriate cut on \alphat. In the Run~1 analyses, a minimum requirement of $\alphat > 0.55$ was imposed, with
%a raised threshold (as high as 0.65) used at low \HT to control against a potential increase in contamination from multijet production
%due to worsening resolution and jet \PT threshold effects.  (The latter effect refers to ``fake'' \mht arising from the rare occurrence
%of multiple soft jets below threshold that are relatively collinear.)The minimum threshold of 0.55 was motivated primarily by the control
%of the multijet background and considered conservative for mid to high regions in \HT, while maintaining good signal acceptance. However,
%improving jet resolutions and a reduced jet threshold effect relative to an increasing \HT scale means that looser \alphat thresholds can be
%used at higher values of \HT.

A useful approximate conversion between \alphat and \mht can be obtained by calculating \alphat, as described by Eq.~\ref{eq:alphat3}, 
while forcing $\dht = 0\gev$. Using this dependence of \alphat as a function of the \HT one can rquired that \mht is roughly constant across all \HT bins. 
The values typically fall in the range $\sim110 < \mht < \sim160\gev$. This approximate levelling of the ``effective'' \mht threshold implies
increasingly tighter requirements against instrumental effects versus \HT, while maximising signala cceptance. 
Table~\ref{tab:alphat-thresholds} shows the expected \alphat thresholds and corresponding ``effective'' \mht thresholds for each \HT bin. 
The \alphat threshold is dependent only on \HT and not on \njet nor \nb that are used to define the event categories.


\begin{table}[h!]
  \caption{\alphat and corresponding ``effective'' \mht (GeV) thresholds versus
    lower bound of \scalht bin. For all \HT bins satisfying $\HT > 800
    \gev$, the direct requirement of $\mht > 130\gev$ is imposed rather
    than a requirement on \alphat. No \alphat requirement is imposed in the
    monojet bins.}
  \label{tab:alphat-thresholds}
  \centering
  \footnotesize
  \begin{tabular}{ lcccccccc }
    \hline
    \hline
    \scalht            & 200       & 250       & 300       & 350       & 400       & 500       & 600       \\
    \hline                                                                                     
    \alphat threshold  & 0.65      & 0.60      & 0.55      & 0.53      & 0.52      & 0.52      & 0.52      \\
    ``Effective'' \mht & $\sim$128 & $\sim$138 & $\sim$125 & $\sim$123 & $\sim$110 & $\sim$138 & $\sim$162 \\
    \hline
    \hline
  \end{tabular}
\end{table}

For Run~2, all signal region bins satisfying $\HT > 800\gev$ are seeded by the single-object \texttt{HLT\_HT800} trigger, which is
expected to be unprescaled. For these high \HT bins, no \alphat threshold is required, which removes the inefficiencies of this
variable for high jet multiplicity events. Instead, the following $\mht >130\gev$ requirement helps to control the multijet background
along with the imposition of $\bdphi > 0.5$.



Further, an additional powerful variable \bdphi is used to suppress multijet contamination due to both instrumental effects and
semi-leptonic heavy-flavour decays with genuine \met in the final state. The variable is determined as follows. The jet-based estimate
of the missing transverse energy, ${\mhtvec}$, is recomputed while ignoring one of the reconstructed jets (the ``test'' jet). The
difference in the azimuthal angle between the recomputed $\mhtvec$ and the ``test'' jet is then determined. This process is repeated for
each jet in the event and the minimum of all the azimuthal differences, \bdphi, is determined. For monojet events, the calculation is 
performed using all jets with $\Pt > 20\gev$. The ``test'' jet whose subtraction from the calculation $\mhtvec$ yields this minimum value, is
identified as the jet that is most likely to have given rise to the missing transverse energy in the event. Events with significant \mht
due to instrumental effects or heavy flavour decays populate the region $\bdphi \approx 0$ and so candidate signal event are accepted
only if they satisfy $\bdphi > 0.5$. The use of the \bdphi and \alphat variables provide an extremely powerful rejection factor against
contamination from multijet events and allow to maintain low jet \PT, \HT, and \mht thresholds, which in turn maximises signal acceptance
for a large range of DM and SUSY models with final states characterised by the presence of significant \met.

To protect against multiple jets failing the $\Et$ threshold or falling out of detector acceptance, the jet-based
estimate of the missing transverse energy, \mht, is compared to the missing transverse energy variable, $\met$, and events with $R_{\rm
  miss}=\mht/\met > 1.25$ are rejected. 
  
Masked regions in the ECAL (which amount to about 1\% of the ECAL channel count)
or HCAL, or by missing instrumentation in the barrel-endcap gap, could cause 
severe energy losses. A data-driven method is developed to identify dead cells. The 
procedure is carried out on events that pass a loose selection of one good primary vertex,
$\njet>1$ and $\scalht>150\gev$. For each identified jet with $\PT > 20 \gev$ in data, the azimuthal angle ($\Delta\phi_{jet}$) between the jet and the 
recomputed ${\mhtvec}$ is determined, in the same way as in the procedure to compute the \bdphi 
variable. The positions of all jets which give $\bdphi < 0.3$ are plotted in an $\eta-\phi$ map. Subsequently, the positions of all jets with
$\pt>20\gev$ are plotted in a second $\eta-\phi$ map. These two maps are then divided to form a 2D ratio map, taking the
first map as the numerator and second as the denominator. Jets pointing to dead cells are likely to give $\bdphi < 0.3$, so the
location of dead cells in $\eta$ and $\phi$ have higher values in this 2D ratio map. Fig



The CSC beam halo filter has been found to be less efficient during the early Run 2 data-taking period compared to the previous run.
Beam halo events manifest themselves as single energy deposits in the calorimeters, which introduces large amounts of ``fake'' \met. This effect is
especially prominent in the signal region monojet category, particularly at $\phi$ coordinates of 0 and $\pi$ because of the tendency of halo particles to
lie within the plane of the LHC ring. 
Such spurious events are suppressed by requiring at least 10\% of the leading jet's energy to originate from charged hadrons, $CHF>0.1$. 

{\bf Summary of signal region selection.} 

The requirements that define the hadronic signal region are summarised
in Table~\ref{tab:sr-selections}.

\begin{table}[h!]
  \topcaption{Summary of the signal region selection criteria, applied
    in addition to the pre-selection summarised in
    Table~\ref{tab:pre-selections}.}
  \label{tab:sr-selections}
  \centering
  \footnotesize
  \begin{tabular}{ ll }
    \hline
    \hline
    Selection             & Requirement                                                    \\
    \hline
    \alphat               & $>$0.52--0.65 (\HT-dependent) for region $200 < \HT < 800\gev$ \\
    \bdphi                & $>0.5$                                                         \\
    \mht/\met             & $<1.25$                                                        \\
    ``Dead ECAL filter''  & (see text)                                                     \\
    ``Beam Halo Filter''  &  $CHF(\textrm{leading jet})>0.1$                                \\

    \hline
    \hline
  \end{tabular}
\end{table}

\subsection{The \texorpdfstring{\mht}{MHT} dimension}

As described above, and as used in Run~1, the analysis takes advantage of three discriminating variables, \njet, \nb, and \HT, to provide
sensitivity to a large range of SUSY (and DM) models. No extrapolation in these variables is performed, with predictions of SM background
yields in the (\njet,\nb,\HT) bins of the signal region based on both observed counts and transfer factors derived from simulated yields in
the corresponding (\njet,\nb,\HT) bins of the control samples. Each prediction is statistically and systematically independent.

In Run~1, for each (\njet,\nb,\HT) bin in the signal region, an extrapolation in the variable \alphat was necessary to obtain
background predictions based on the muon control samples, which did not impose any \alphat requirement. No extrapolation in \alphat was
performed for the photon control sample, which used the same \alphat requirements as the signal region. The \alphat requirements used in
Run~1 for the signal region correspond loosely to \mht thresholds inthe range $\sim$130 to $\sim$500\gev depending on the \HT
bin. Uncertainties in this extrapolation were determined through closure tests with respect to data, including one dedicated to the
\alphat extrapolation, plus additional cross checks.

In Run~2, we additionally bin event counts in the signal regionaccording to the variable \mht in order to provide further
discriminating power between any potential signal and the SM background counts. Hence, while no extrapolation is performed in
\njet, \nb, nor \HT, the analysis relies on information obtainedfrom simulation to extrapolate from counts (integrated over \mht) in
the control samples to a predicted distribution in \mht for each corresponding (\njet,\nb,\HT) bin in the signal region.

The \mht dimension is included in the likelihood model using templates determined per (\njet,\nb,\HT) bin from simulation. An associated
normalisation nuisance is determined from closure tests between simulation and data, as described in
Sec.~\ref{sec:systematics}. Alternative templates are used to encodethe systematic uncertainty in the \mht distribution obtained from
simulation. 

The templates use \mht bins of 50\gev in width. A metric is used to determine the threshold of the final \mht bin used by the
templates. This metric is currently based on requiring a minimum number of both observed counts in the {\it data control samples} and
(unweighted) simulated events with \mht values higher than the finalbin threshold. While the counts in the (\njet~,\nb~,\HT) bins of each
data control sample are not be binned according to \mht (as in the signal region), the former requirement ensures that there are
sufficient events in each (\njet~,\nb~,\HT) bin of the control samplesto probe for potential systematic effects {\it across all bins in
  \mht} using closure tests between simulation and data, as described in Sec.~\ref{sec:systematics}. Hence, a data-driven validation for
potential biases and on the systematic uncertainty in the \mht template is possible. The latter requirement minimises the statistical
uncertainties associated with the finite number of simulated events for the various SM background processes. Both requirement are based on
counts according to \njet and \nb but inclusive with respect to \nb.
