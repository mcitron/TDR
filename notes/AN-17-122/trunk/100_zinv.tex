%%____________________________________________________________________________||

\section{\texorpdfstring{\znunu\ + jets}{Zinv} background estimation}
\label{sec:zinv}

The irreducible background contribution from the \znunu\ + jets
process can be sizeable, and often the dominating contribution, in
many event categories of the signal region. Table~\ref{} (App.~\ref{})
shows 

Once all the signal region selection requirements have been imposed,
the contribution from QCD multijet events is expected to be
negligible, as demonstrated in Sec.~\ref{sec:qcd}. 
In the absence of multijet events, the background counts in the signal region arise from
SM processes with significant \met in the final state. In events with
low statistics of jets and b-quark jets, the largest backgrounds with
genuine \met are from the associated production of W or Z bosons with
jets, followed by either the weak decays \znunu or \wtaunu, where the
$\tau$ decays hadronically and is identified as a jet; or by leptonic
decays that are not rejected by the dedicated electron or muon
vetoes. The veto of events containing isolated tracks is efficient at
further suppressing these backgrounds as well as the single-prong
hadronic decay of the tau lepton. At higher jet and b-quark jet
multiplicities, top quark production followed by semileptonic weak top
quark decay becomes important.  Residual contributions from processes
such as single-top-quark, $\ttbar$V or $\ttbar$H, diboson, and
Drell-Yan production are also expected. These SM processes are
collectively referred to as the non-multijet backgrounds.

\subsection{Additional corrections to simulated samples}
\label{sec:mc-corrections}

The simulated samples are normalised using the most accurate cross
section calculations currently available, usually with
next-to-leading-order (NLO) accuracy~\ref{sec:datasets}. 
To model the effect of pileup, the simulated events are generated with a nominal distribution
of pp interactions per bunch crossing, which are then reweighted
to match the pileup distribution as measured in data, as described in Section~\ref{sec:pileup-reweighting}.

Additional re-weighting procedures applied to the simulated samples are described in the following.

\subsubsection{Normalisation corrections from a \mht sideband}
\label{sec:sideband-corrections}

In the high-\scalht, high-\etmiss corner of the phase space used in this search, the normalisations of the MC samples do not necessarily agree with the observation. 
Moreover, the cross section is known only to a limited number of perturbative orders and additional corrections could be in principle sizeable. \\
The analysis strategy for the background predictions is built in such a way to be mildly, if not negligibly, dependent on these corrections. 
The backgrounds are estimated from control regions in data, and the effect of cross section corrections on the transfer factors is expected to largely cancel out, 
because the background composition is very similar between the signal region and the control regions used to estimate each background. \\
However, the ``data-driven'' tests described in Sec.~\ref{sec:closure-tests} would benefit from a better control of the normalisation of MC samples, 
since more aggressive extrapolations are carried on there with respect to the background predictions in the analysis. 

In this section a procedure is described to derive process-dependent ``sideband corrections'' 
by means of a likelihood fit using the data in the control regions. 
The sideband corrections are derived after all the other corrections to the MC and data are applied, 
such as trigger efficiency, data/MC scale factors (b-tag, lepton ID/isolation, etc.) and jet energy scale. 
The sideband corrections are applied and propagated to all the steps of the analysis.\\
No uncertainty is considered for these corrections as any inaccuracy is already accounted 
for in the data-driven tests described in Sec.~\ref{sec:closure-tests} and would result in inflated systematics. 

To take advantage of the full phase space of the sidebands a simultaneous 
fit is used to derive the corrections for \wj, \zj, \ttbar, using the $100<\mht<130$ GeV sideband. 
The sideband is binned identically to the control region in \njet, \nb and \scalht and a floating 
parameter per relevant process encodes the correction for that process (fully correlated across all bins).
The \wj and \ttbar processes are mainly constrained by the \mj sideband while the \zj process is 
constrained by the \mmj sideband. The values of the corrections and uncertainties
given by the fit are shown in Table~\ref{tab:sbCorrsFromFit}.\\
The correction derived for \zj is also applied to the \znunu sample. 

\begin{table}[!h]
  \scriptsize
  \centering
  \topcaption{Cross section corrections for SM backgrounds derived with fit to sidebands in data.}
  \label{tab:sbCorrsFromFit}
  \begin{tabular}
    {cllc}
    \hline\hline
    \textbf{Process} & \textbf{Sideband} & \textbf{Selection} & \textbf{Corrrection} \\
    \hline
    \wj & $100 < \mht < 130 \, \mathrm{GeV}$ & \mj& $1.13 \pm 0.01$ \\
    \zj & $100 < \mht < 130 \, \mathrm{GeV}$ & \mmj& $1.08 \pm 0.01$ \\
    \ttbar + jets & $100 < \mht < 130 \, \mathrm{GeV}$ & \mj, \mmj  & $0.91 \pm 0.01$ \\
    \hline \hline
  \end{tabular}
\end{table}

\subsubsection{Estimate of non-prompt contribution in \gj events}
\label{sec:photon-purity}

The contribution from non-prompt (``fake'') photons in the \gj control 
region~(\ref{sec:photoncontrolSelection}) is expected to be small after
the tight ID and acceptance requirements~(\ref{sec:photon-id}). 
Nevertheless a data-driven estimation of the non-prompt photons is
employed. 

In the previous version of the analysis, the QCD Monte Carlo samples
were reweighted using data$/$MC corrections derived in a fake-enriched
$\sigma_{i\eta i\eta}$ sideband as a function of photon \Pt. This
required subtracting the prompt photon contribution in the sideband
using simulation, which although small for the most part, was quite
sizeable in analysis bins with large \njet and \HT (up to 50-70\%).
Therefore this method was not sufficiently robust, and an alternative
method (a ``template fit'' method) has been employed as described 
in the following.
%Both methods agree within statistical uncertainties

In this method, a template maximum likelihood fit is performed to the
charged hadron isolation distribution of the photons in data. This
allows both the fake and prompt components to be constrained
simultaneously. The templates for the non-prompt photons are either
taken from simulation or are extracted
from the fake-enriched $\sigma_{i\eta i\eta}$ sideband (0.011 $< \sigma_{i\eta i\eta} <$ 0.02).
The templates for the prompt photons is obtained via the ``random
cone'' method \cite{random-cone}, in which the charged hadron
isolation distribution is constructed by randomising the direction of
prompt photons in data and recomputing their isolation value.

The fit is performed over a 0-10~GeV range in isolation, and the
resulting best fit value for the fraction of fakes is used to compute
an estimate of the non-prompt yield in the nominal \gj control region
(chHadIso $<$ 0.202). The fit is performed twice, using templates for
the non-prompt component from simulation and from the $\sigma_{i\eta
i\eta}$ sideband. The final value for the yield is taken as the
average of those obtained using these two schemes. The difference
halved is taken as a systematic uncertainty.
% fake templates integrated over HT

Two example fits are shown in Fig.~\ref{fig:photonTemplateFits}. The
relative conribution of non-prompt photons in the \gj control region 
is shown as a function of (\njet,\HT) in Fig.~\ref{fig:photon-purities}.
The study confirms a high photon purity for the selection applied
in this analysis, with a typical fake photon component between 1-5\%.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/photonpurity/fakeFit_eq2j_600_prefit} ~
  \includegraphics[width=0.45\textwidth]{figures/photonpurity/fakeFit_eq2j_600_postfit} \\
  \includegraphics[width=0.45\textwidth]{figures/photonpurity/fakeFit_ge5j_900_prefit} ~
  \includegraphics[width=0.45\textwidth]{figures/photonpurity/fakeFit_ge5j_900_postfit}
  \caption{\label{fig:photonTemplateFits} 
  Example prefit (left) and postfit (right) charged hadron isolation
  distributions in the (\njet$=2$, \HT$=600-900$) (top) and 
  (\njet$=5$, \HT$=900-1200$) (bottom) bins.}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/photonpurity/fake_rate_MC}
  \includegraphics[width=0.45\textwidth]{figures/photonpurity/fake_rate_see}
  \caption{\label{fig:photon-purities} 
  The relative contribution of non-prompt photon as a function
  of (\njet,\HT), when using fakes templates from simulation (left)
  and from the $\sigma_{i\eta i\eta}$ sideband.}
\end{figure}


\subsubsection{Correction to \texorpdfstring{\gj}{photon+jets} cross section}
\label{sec:gj-kfactor}

The \gj cross section used in this analysis is calculated at leading
order, unlike other processes like \ttj (next-to-next-to leading
order) or \zj and \wj (next-to leading order).  A data-driven
procedure is developed to derive a ``k-factor'' for the \gj cross
section by comparing the yields in the \gj and \mmj control regions.
An inclusive correction of 1.39 is derived from the ratio of the event
yields in the two
control regions and is applied to the \gj MC sample. \\
In order to assess the uncertainty on this correction, the systematic
sources affecting the acceptance in the \mmj control region are varied
and the effect on the event yields is derived.  They include lepton
trigger, ID, isolation, tracking efficiency and jet energy
corrections.  The total systematic uncertainty, obtained by summing in
quadrature each independent variation, is 4\% and it's propagated to
the likelihood fit.  Residual discrepancy in the $Z/\gamma$ ratio as a
function of \scalht and \njet are assessed through the data-driven
test described in Sec.~\ref{sec:closure-tests}.


\subsection{The ``transfer factor'' method}
\label{sec:ewk-method}

The method used to estimate the aforementioned SM background
contributions in the hadronic signal region relies on the use of a
transfer factor (TF) determined from MC samples to transform the
observed yield in a given \scalht, jet (\njet) and b-tag (\nb)
multiplicity bin of a control sample, $\nobs^{\rm
  control}(\njet,\nb,\scalht)$, into a predicted yield for the
corresponding bin of the hadronic signal region, $\npre^{\rm
  signal}(\njet,\nb,\scalht)$. The choice of \njet and \nb~event
categorisation and \scalht binning in the control samples is identical
to that for the signal region, as defined in
Sec.~\ref{sec:selection}. 

Each transfer factor is simply a ratio of the yields obtained from MC
simulation for the same bin of the signal region and a given control
sample:

\begin{equation}
  \label{equ:tf-ratio}
  {\rm TF} = \frac{N_{\rm MC}^{\rm signal}(\njet,\nb,\scalht)}{N_{\rm
      MC}^{\rm control}(\njet,\nb,\scalht)} 
\end{equation}

In this way, predictions of background counts from SM processes can be
made based on the various control samples:

\begin{equation}
  \label{equ:pred-method}
  \npre^{\rm signal}(\njet,\nb,\scalht) = \frac{N_{\rm MC}^{\rm
      signal}(\njet,\nb,\scalht)}{N_{\rm MC}^{\rm
      control}(\njet,\nb,\scalht)} \times \nobs^{\rm
    control}(\njet,\nb,\scalht)   
\end{equation}

When constructing the transfer factors, the MC expectations for the
following SM processes are considered: W + jets ($N_{\rm W}$), \ttbar
+ jets ($N_{\ttbar}$), \znunu\ + jets ($N_{\znunu}$), DY + jets
($N_{\mathrm DY}$), \gj ($N_\gamma$), single top + jets
production via the $s$, $t$, and $tW$-channels ($N_{\rm top}$), $WW+$~jets, $WZ~+$~jets, and $ZZ + \textrm{jets}$ ($N_{\rm di-boson}$), and $\ttbar$V or
$\ttbar$H ($N_{\rm {\ttbar}X}$). Details on the MC
samples used are given in Sec.~\ref{sec:datasets}. All MC samples
are normalised to the integrated luminosity of the appropriate data
sample.

The selection criteria for the data control samples closely resemble
those for the signal region, differing mainly through the use of a
lepton or photon object {\it tag} (that is ignored in the calculation
of jet-based kinematic variables such as \scalht, \mht, \alphat, \etc)
and minimal additional kinematic requirements (\eg invariant or
transverse mass windows) to obtain W, Z, and \ttbar-enriched event
samples. The same selection criteria are designed to suppress signal
contamination in the control samples so that unbiased data-driven
estimates for the SM backgrounds in the signal region can be
made. More detail on the selection criteria can be found in Sec.~\ref{sec:selection}.

The transfer factors account for differences in cross sections and
branching ratios, acceptance and reconstruction efficiencies, and/or
kinematic requirements between the signal and control regions. Any
dependence on \njet, \nb, or \HT is largely attributable to
differences in acceptance due to the presence or otherwise of \alphat
or \mht requirements.

Many systematic effects are expected to cancel largely in the transfer
factor. However, a systematic uncertainty is assigned to each transfer
factor to account for theoretical uncertainties and effects such as
the mismodelling of kinematics (\eg acceptances) and instrumental
effects (\eg reconstruction efficiencies).

In the end, a fitting procedure that provides the final result is
defined formally by the likelihood model described in
Sec.~\ref{sec:likelihood}. In summary, the observation in each bin
(defined in terms of the variables \njet, \nb, and \scalht) of the
signal sample is modelled as Poisson-distributed about the sum of a SM
expectation (and a potential signal contribution). The components of
this SM expectation are related to the expected yields in the control
samples via transfer factors derived from simulation. The observations
in each bin (again defined by \njet, \nb, and \scalht) of the control
samples are similarly modelled as Poisson-distributed about the
expected yields for each control sample. In this way, for a given
bin, the observed yields in the signal and control samples are
connected via the transfer factors derived from simulation. 
%% The transfer factors are shown in Tables~\ref{tab:tf_mu_zinv_sym}-
%% \ref{tab:tf_mumu_zinv_mono}. The procedure to determine the systematic
%% uncertainties associated with these transfer factors is described in
%% Sec.~\ref{sec:systematics}. 
The procedure to determine the systematic uncertainties associated
with these transfer factors is described in
Sec.~\ref{sec:systematics}.


%% The transfer factors are shown in tables below.

%% \input{tables/dataLumi/tf_mu_zinv_sym.tex}
%% \input{tables/dataLumi/tf_mu_zinv_asym.tex}
%% \input{tables/dataLumi/tf_mu_zinv_mono.tex}
%% \input{tables/dataLumi/tf_gj_zinv_sym.tex}
%% \input{tables/dataLumi/tf_gj_zinv_asym.tex}
%% \input{tables/dataLumi/tf_gj_zinv_mono.tex}
%% \input{tables/dataLumi/tf_mu_ttw_sym.tex}
%% \input{tables/dataLumi/tf_mu_ttw_asym.tex}
%% \input{tables/dataLumi/tf_mu_ttw_mono.tex}
%% \input{tables/dataLumi/tf_mumu_zinv_sym.tex}
%% \input{tables/dataLumi/tf_mumu_zinv_asym.tex}
%% \input{tables/dataLumi/tf_mumu_zinv_mono.tex}

%% \clearpage

\subsection{Adding the \mht dimension}

The aforementioned description of the TFs provide an estimate of the
total SM background as a function of the (\njet,\nb,\HT) bin that is
integrated over \mht. However, the analysis takes advantage of \mht
distribution obtained from simulation. This information is propagated
to the likelihood model via an \mht template per (\njet,\nb,\HT) bin,
which is equivalent to dicing the numerator of the TF according to
\mht, \ie $N_{\rm MC}^{\rm signal}(\njet,\nb,\scalht,\mht)$. In this
regard, the TFs described above provide an estimate of the
normalisation for each \mht template.

%\subsection{Data control samples used in the method}
%
%To estimate the contributions from backgrounds with genuine missing
%transverse momentum, three data control regions are used, which are
%binned identically to the signal region: \mj, \mmj and \gj.  Their
%definitions are provided in Sec.~\ref{sec:selection}. The selection
%criteria for these control regions are defined such that any potential
%contamination from new physics processes or QCD multijets is
%negligible.

%% In previous versions of this analysis, the \mmj and \gj control
%% samples are used to predict the \znunu +jets background. We plan to
%% extend this approach by relying on all (and not just a sub-set of)
%% relevant control samples to predict the two dominant components of the
%% total SM background (\wj and \ttbar, or \znunu + jets). Specifically,
%% we are using the \mj sample to predict the \wj and \ttbar backgrounds
%% (across all \nb bins) and up to three samples comprising \zmmj,
%% \gj and \wmj to predict the \znunu + jets background for events
%% containing exactly zero or one b-tagged jets. Any correlations are
%% appropriately handled by the likelihood model (via the
%% \texttt{Combine} tool).

%% The predictions of the \znunu + jets background based on the \zmmj
%% control samples exhibits significantly larger statistical
%% uncertainties at high \njet, \nb, \scalht, or \mht due to lower event
%% counts arising from the lower Z cross section (w.r.t. \gj and
%% \wj). Regardless, these samples are included in the likelihood fit
%% to provide additional confidence in the control of the \znunu + jets
%% background.

%% Concerning the use of $W$-enriched samples to predict the \znunu +jets
%% background, we have studied this approach
%% in detail with the 8\TeV dataset. Based on the outcome of these studies,
%% we have decided to proceed with this approach, as part
%% of the baseline likelihood description. Studies were
%% based on data-driven tests with 8\TeV data
%% (as described in Sec.~\ref{sec:closure-tests}). Closure tests are a critical
%% tool to determine which samples can be used to predict the SM
%% background components. In particular, we have studied the effect of
%% new closure tests designed to test the $W$-enriched to $Z$-enriched
%% extrapolation, specifically \mj to \gj, \mj to \mmj
%% and $\mu^{+}$ to $\mu^{-}$ closure tests. 
%% If further studies of the closure tests with $13\tev$
%% data suggest that using the \mj control region to predict the \znunu
%% background is not feasible, we will revert back to the approach
%% used in Run~I analysis (\ie relying solely on the \zll and \gj
%% samples). Early investigations suggest there are not any major problems.
%% %The closure tests provide important event samples for probing the
%% %accuracy of the simulation modelling implicit in the transfer factors.
%% %Specific examples include ``\mj to predict \mmj'' and ``zeej to
%% %predict \gj'' with events containing exactly zero or one b-tagged
%% %jets. The former test relies on a \wmj-enriched sample to predict
%% %yields in the \zmmj sample (in the presence of some ``\ttbar
%% %contamination'' for events with $\nb = 1$). The latter test is a
%% %consistency check between a dilepton and a \gj sample (as done in
%% %previous iterations of the analysis).
%% Most importantly, we retain full flexibility in our approach and the
%% control samples used to predict the various background components.

%% Currently, the projected sensitivity, as described in the current
%% version of this note, is based on predictions of SM background
%% components made from control regions as follows. For events containing
%% exactly zero or one b-tagged jets, the \mj (enriched in \wej), \gj and
%% \mmj control samples are used to estimate the irreducible \znunu + jets
%% background, while the \mj control sample is used to estimate all
%% remaining SM processes (predominately \wj and \ttbar). For events
%% containing two or more b-tagged jets, the \mj sample is
%% used to predict the total SM background (dominated by \ttbar).

